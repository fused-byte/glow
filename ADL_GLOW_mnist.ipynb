{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADL_GLOW_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE66NzXhj9Me",
        "outputId": "e9bcef40-d5a7-47d4-90dd-bd18f26eb49e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFQ1lI_G74jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db74f6a6-385a-4009-df24-2dce4353b557"
      },
      "source": [
        "#!pip install tensorflow-gpu==2.3.0rc0 tensorflow_probability\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow import keras\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "print('tensorflow: ', tf.__version__)\n",
        "print('tensorflow-probability: ', tfp.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow:  2.3.0\n",
            "tensorflow-probability:  0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3NpXIHREzjv",
        "outputId": "f0396ab8-77b4-4248-945b-43c5d2771d72"
      },
      "source": [
        "#GPU check\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi2UxUOW3bec"
      },
      "source": [
        "**LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRE1bUW63e6g"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#dataset = 'cifar10'\n",
        "\n",
        "def make_iterator(flow):\n",
        "    def itertor():\n",
        "        return flow.next()\n",
        "\n",
        "    return itertor\n",
        "\n",
        "\n",
        "def get_data(dataset, batch_size):\n",
        "    if dataset == \"mnist\":\n",
        "        from keras.datasets import mnist\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        y_train = y_train.reshape([-1])\n",
        "        y_test = y_test.reshape([-1])\n",
        "\n",
        "        x_train = np.lib.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'minimum')\n",
        "        x_test = np.lib.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'minimum')\n",
        "\n",
        "        x_train = np.tile(np.reshape(x_train, (-1, 32, 32, 1)), (1, 1, 1, 3))\n",
        "        x_test = np.tile(np.reshape(x_test, (-1, 32, 32, 1)), (1, 1, 1, 3))\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "\n",
        "        x_train = (x_train /(255.0/2)) - 1\n",
        "        x_test = (x_test /(255.0/2)) - 1\n",
        "\n",
        "    elif dataset == \"cifar10\":\n",
        "        (train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
        "        x_train = tf.cast(train_x, dtype=tf.float32)\n",
        "        x_test = tf.cast(test_x, dtype=tf.float32)\n",
        "        x_train = (train_x / (255.0 / 2)) - 1\n",
        "        x_test = (test_x /(255.0/2)) - 1\n",
        "        y_train = train_y.reshape([-1])\n",
        "        y_test = test_y.reshape([-1])\n",
        "\n",
        "    datagen_test = ImageDataGenerator()\n",
        "    datagen_train = ImageDataGenerator(width_shift_range=0.1,\n",
        "                                       height_shift_range=0.1)\n",
        "\n",
        "    datagen_train.fit(x_train)\n",
        "    datagen_test.fit(x_test)\n",
        "\n",
        "    train_flow = datagen_train.flow(x_train, y_train, batch_size)\n",
        "    test_flow = datagen_test.flow(x_test, y_test, batch_size, shuffle=True)\n",
        "\n",
        "    train_iterator = make_iterator(train_flow)\n",
        "    test_iterator = make_iterator(test_flow)\n",
        "\n",
        "    return train_iterator, test_iterator\n",
        "\n",
        "def get_data_alt(batch_shuffle_size, batch_size):\n",
        "    from keras.datasets import mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    @tf.function\n",
        "    def _parse_function(img, label):\n",
        "        feature = {}\n",
        "        img = tf.pad(img, paddings=[[2, 2], [2, 2]], mode=\"CONSTANT\")\n",
        "        img = tf.expand_dims(img, axis=-1)\n",
        "        img = tf.reshape(img, [32, 32, 1])\n",
        "        # print(img.shape)\n",
        "        img = tf.tile(img, ( 1, 1, 3))\n",
        "\n",
        "        img = tf.cast(img, dtype=tf.float32)\n",
        "        img = (img / (255.0 / 2)) - 1\n",
        "        # img = img/255.0\n",
        "        feature[\"img\"] = img\n",
        "        feature[\"label\"] = label\n",
        "        return feature\n",
        "\n",
        "\n",
        "    train_dataset_raw = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(\n",
        "            _parse_function\n",
        "            )\n",
        "    test_dataset_raw = tf.data.Dataset.from_tensor_slices((x_test, y_test)).map(\n",
        "            _parse_function\n",
        "            )\n",
        "    train_dataset = train_dataset_raw.shuffle(batch_shuffle_size).batch(batch_size)\n",
        "    test_dataset = test_dataset_raw.shuffle(batch_shuffle_size).batch(batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYg5GHWX3hXh"
      },
      "source": [
        "**MODEL LAYERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-y7_wVo3pI4"
      },
      "source": [
        "#Activation Normalization layer\n",
        "class ACT(tfp.bijectors.Bijector):\n",
        "\n",
        "    # Ref - https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector\n",
        "    def __init__(self, channels, validate_args=False, name='actnorm', log_scale_factor = 1.0):\n",
        "        super(ACT, self).__init__(\n",
        "          validate_args=validate_args,\n",
        "          forward_min_event_ndims=1,\n",
        "          name=name)\n",
        "        self.log_scale_factor = log_scale_factor\n",
        "        self.initialized = False\n",
        "        self.log_scale = tf.Variable( tf.random.normal([channels]), name=name + \"/logScale\")\n",
        "        self.bias = tf.Variable(tf.random.normal([channels]),name=name + \"/bias\")\n",
        "\n",
        "    def actnorm_mean_var(self, x):\n",
        "        mean = tf.math.reduce_mean(x, axis=[0, 1, 2])\n",
        "        var = tf.math.reduce_mean((x-mean) ** 2, axis=[0, 1, 2])\n",
        "        stdVar = tf.math.sqrt(var) + 1e-6\n",
        "        log_scale = tf.math.log(1./ stdVar / self.log_scale_factor) * self.log_scale_factor\n",
        "        # print('log scale shape: ', log_scale.shape)\n",
        "        self.bias.assign(-mean)\n",
        "        self.log_scale.assign(log_scale)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        if not self.initialized:\n",
        "            self.actnorm_mean_var(x)\n",
        "            self.initialized = True\n",
        "        return (x + self.bias) * tf.exp(self.log_scale)\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        if not self.initialized:\n",
        "            self.actnorm_mean_var(y)\n",
        "            self.initialized = True\n",
        "        return y * tf.exp(-self.log_scale) - self.bias\n",
        "\n",
        "    ### Formula the same as in paper but this is producing values like -1825236.9 something\n",
        "    ### for log_det_jcb which seems like a pretty huge value for the below test values. \n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        shape = x.get_shape()\n",
        "        # log_det = int(shape[1]) * int(shape[2])\n",
        "        # print(log_det)\n",
        "        # return log_det * tf.reduce_sum(self.log_scale)\n",
        "        return tf.reduce_sum(self.log_scale)\n",
        "\n",
        "    def _inverse_log_det_jacobian(self, y):\n",
        "        # shape = y.get_shape()\n",
        "        # log_det = int(shape[1]) * int(shape[2])\n",
        "        # return - log_det * tf.reduce_sum(self.log_scale)\n",
        "        # print(self.log_scale)\n",
        "        # print(tf.reduce_sum(self.log_scale))\n",
        "        #print('Test inverse value: ', tf.reduce_sum(self.log_scale))\n",
        "        return -tf.reduce_sum(self.log_scale)\n",
        "\n",
        "def test_actnorm():\n",
        "    actnorm = ACT(4)\n",
        "    x = tf.random.normal([100, 16, 16, 4]) + 100\n",
        "    y = actnorm.forward(x)\n",
        "    z = actnorm.inverse(y)\n",
        "    print('input: x', tf.reduce_mean(x, axis=[0, 1, 2]).numpy())\n",
        "    print('output: y', tf.reduce_mean(y, axis=[0, 1, 2]).numpy())\n",
        "    print('inverse: z', tf.reduce_mean(z, axis=[0, 1, 2]).numpy())\n",
        "    print('log_det_jacobian: ', actnorm.forward_log_det_jacobian(y, event_ndims=3).numpy()/256) ## this prints very large values\n",
        "    print('log_det_jacobian: ', actnorm.inverse_log_det_jacobian(y, event_ndims=3).numpy()/256)\n",
        "    print(tf.shape(y))\n",
        "    print(tf.size(tf.shape(y)))\n",
        "    print('trainable variables',actnorm.trainable_variables)\n",
        "\n",
        "# test_actnorm()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HS0CKyg378C"
      },
      "source": [
        "#Affine Coupling layer\n",
        "class NN(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "            output_shape,\n",
        "            activation=\"relu\",\n",
        "            kernel_size = [[3,3], [1,1]],\n",
        "            n_hidden=[512,512], \n",
        "            stride = [1,1],\n",
        "            padding=\"SAME\",\n",
        "            name=\"nn\"):\n",
        "        if name:\n",
        "            super(NN,self).__init__(name=name)\n",
        "        else:\n",
        "            super(NN,self).__init__()\n",
        "\n",
        "        ### official implementation uses different initialisation for weights for the first 2 layers.\n",
        "        ### random_normal_initialiser. ours is Xavier. \n",
        "        ### We are using Xavier because the paper doesn't talk about their initialisation method benefits as such \n",
        "        # and Xavier is used in most of the networks. However, we dont know the significance of this difference.\n",
        "        self.conv1 = keras.layers.Conv2D(n_hidden[0], kernel_size = kernel_size[0], strides=stride, padding=padding, activation=activation, name=name+\"/conv_1\")\n",
        "        self.conv2 = keras.layers.Conv2D(n_hidden[1], kernel_size = kernel_size[1], strides=stride, padding=padding, activation=activation, name=name+\"/conv_2\")\n",
        "        self.log_s_layer = keras.layers.Conv2D(output_shape, kernel_size=[3,3], strides = stride,  kernel_initializer=\"zeros\", padding=padding, activation='tanh', name=name+\"/conv_log_s\")\n",
        "        self.t_layer = keras.layers.Conv2D(output_shape, kernel_size=[3,3], strides = stride,  kernel_initializer=\"zeros\", padding=padding, name=name+\"/conv_t\")\n",
        "        # self.conv3 = keras.layers.Conv2D(output_shape, kernel_size=[3,3], strides = stride, padding=padding, name=name+\"/conv_3\")\n",
        "\n",
        "    def call(self,x):\n",
        "        # print(\"weights: \", self.conv1.weights)\n",
        "        # print(\"conv1 input: \", x.shape)\n",
        "        x = self.conv1(x)\n",
        "        # print(\"conv2 input: \", x.shape)\n",
        "        x = self.conv2(x)\n",
        "        # print(\"conv3 input: \", x.shape)\n",
        "        log_s = self.log_s_layer(x)\n",
        "        t = self.t_layer(x)\n",
        "        # print(\"output: \", x.shape)\n",
        "        return log_s, t\n",
        "\n",
        "\n",
        "class ACL(tfp.bijectors.Bijector):\n",
        "    def __init__(self, output_shape, \n",
        "            forward_min_event_ndims = 3, \n",
        "            validate_args=False,\n",
        "            name=\"acl\",\n",
        "            **kwargs):\n",
        "\n",
        "        super(ACL, self).__init__(\n",
        "            validate_args=validate_args,\n",
        "            forward_min_event_ndims=forward_min_event_ndims,\n",
        "            name=name)\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "        self.nn_obj = NN(self.output_shape[-1]//2, name=name+\"/NN\", **kwargs)\n",
        "        k_inp = self.output_shape.copy()\n",
        "        k_inp[-1] = self.output_shape[-1] // 2\n",
        "        \n",
        "        x = tf.keras.Input(k_inp)\n",
        "        # print(\"keras input: \", x.shape)\n",
        "        log_s, t = self.nn_obj(x)\n",
        "        self.nn = tf.keras.Model(x, [log_s, t], name=self.name + \"/nn\")\n",
        "        \n",
        "    def _forward(self, x):\n",
        "        # print()\n",
        "        x_a, x_b = tf.split(x, 2, axis = -1)\n",
        "        y_b = x_b\n",
        "        log_s, t = self.nn(x_b)\n",
        "        # h = self.nn(x_b)\n",
        "        # t = h[:,:,:,0::2]\n",
        "        # scale = keras.activations.sigmoid(h[:,:,:,1::2] + 2.)\n",
        "        # log_s = keras.activations.tanh(h[:,:,:,1::2])\n",
        "        scale = tf.exp(log_s)\n",
        "        y_a = scale * x_a + t\n",
        "        y = tf.concat([y_a,y_b], axis=-1)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        y_a, y_b = tf.split(y, 2, axis = -1)\n",
        "        # print('This is y_b shape : ', y_b.shape)\n",
        "        # h = self.nn(y_b)\n",
        "        # t = h[:,:,:,0::2]\n",
        "        log_s, t = self.nn(y_b)\n",
        "        # scale = keras.activations.sigmoid(h[:,:,:,1::2] + 2.)\n",
        "        # log_s = keras.activations.tanh(h[:,:,:,1::2])\n",
        "        scale = tf.exp(log_s)\n",
        "        x_a = (y_a - t)/scale\n",
        "        x_b = y_b\n",
        "        x = tf.concat([x_a, x_b], axis = -1)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        _ , x_b = tf.split(x, 2, axis = -1)\n",
        "        log_s, _ = self.nn(x_b)\n",
        "        # h = self.nn(x_b)\n",
        "        # log_s = keras.activations.tanh(h[:,:,:,1::2])\n",
        "        scale = tf.math.exp(log_s)\n",
        "        # scale = keras.activations.sigmoid(h[:,:,:,1::2] + 2.)\n",
        "        # log_s = tf.math.log(scale)\n",
        "\n",
        "        # print(\"log scale values\", scale)\n",
        "        # print(\"test: \", K.eval(tf.reduce_sum(log_s, axis = [1,2,3])))\n",
        "        return tf.reduce_sum(log_s)\n",
        "\n",
        "\n",
        "def nn_test():\n",
        "    nn = NN(4, name=\"NN\")\n",
        "    x = tf.random.normal([3, 32, 32, 4])\n",
        "    x = tf.keras.Input([32,32,2])\n",
        "    print(x.shape)\n",
        "    log_s = nn(x)\n",
        "    print(log_s)\n",
        "    # Non trainable params: -> Batch Normalization's params\n",
        "    tf.keras.Model(x, [log_s], name=\"nn_test\").summary()\n",
        "\n",
        "# nn_test()\n",
        "\n",
        "def realnvp_test():\n",
        "    realnvp = ACL(output_shape=[16, 16, 4], n_hidden=[256, 256])\n",
        "    x = tf.keras.Input([16, 16, 4])\n",
        "\n",
        "    y = realnvp.forward(x)\n",
        "    print(\"trainable_variables :\", len(realnvp.trainable_variables))\n",
        "    print('trainable variable NN: ', len(realnvp.nn.trainable_variables))\n",
        "    \n",
        "    flow = tfd.TransformedDistribution(\n",
        "        event_shape=[16, 16, 4],\n",
        "        distribution=tfd.Normal(loc=0.0, scale=1.0),\n",
        "        bijector=realnvp,\n",
        "    )\n",
        "    x = flow.sample(5)\n",
        "    print(x.shape)\n",
        "    # y = realnvp.inverse(x)\n",
        "    log_prob = flow.log_prob(y)\n",
        "    print(realnvp.inverse_log_det_jacobian(x, event_ndims=3).numpy() )\n",
        "    print(\n",
        "        x.shape,\n",
        "        # y.shape,\n",
        "        log_prob,\n",
        "        # -tf.reduce_mean(log_prob),\n",
        "        # -tf.reduce_mean(flow.distribution.log_prob(x)),\n",
        "        # -tf.reduce_mean(\n",
        "        #     flow.bijector.forward_log_det_jacobian(\n",
        "        #         x, event_ndims=flow._maybe_get_static_event_ndims()\n",
        "        #     )\n",
        "        # ),\n",
        "        # -tf.reduce_mean(flow._log_prob(x)),\n",
        "        # flow._finish_log_prob_for_one_fiber(\n",
        "        #     y,\n",
        "        #     x,\n",
        "        #     flow.bijector.forward_log_det_jacobian(\n",
        "        #         x, event_ndims=flow._maybe_get_static_event_ndims()\n",
        "        #     ),\n",
        "        #     flow._maybe_get_static_event_ndims(),\n",
        "        #\n",
        "        # ),\n",
        "        # tf.reduce_sum(flow.distribution.log_prob(\n",
        "        #     flow._maybe_rotate_dims(x, rotate_right=True)),\n",
        "        #               axis=flow._reduce_event_indices)\n",
        "    )\n",
        "\n",
        "#realnvp_test()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QaHFVFQ39i5"
      },
      "source": [
        "#Blockwise module\n",
        "class Blockwise(tfb.Bijector):\n",
        "    def __init__(self,\n",
        "                 bijectors : list,\n",
        "                 block_sizes : list = None,\n",
        "                 validate_args=False,\n",
        "                 name=\"blockwise\"):\n",
        "        super(Blockwise, self).__init__(\n",
        "            forward_min_event_ndims = 3,\n",
        "            validate_args=validate_args,\n",
        "            name=name\n",
        "        )\n",
        "        self._bijectors = bijectors\n",
        "        self._block_sizes = block_sizes\n",
        "\n",
        "    @property\n",
        "    def bijectors(self):\n",
        "        return self._bijectors\n",
        "\n",
        "    @property\n",
        "    def block_sizes(self):\n",
        "        return self._block_sizes\n",
        "\n",
        "    def _forward(self, x):\n",
        "        split_x = (tf.split(x, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None \n",
        "                   else tf.split(x, self.block_sizes, axis=-1))\n",
        "        split_y = [b.forward(x_) for b, x_ in zip(self.bijectors, split_x)]\n",
        "        y = tf.concat(split_y, axis=-1)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        split_y = (tf.split(y, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None \n",
        "                   else tf.split(y, self.block_sizes, axis=-1))\n",
        "        split_x = [b.inverse(y_) for b, y_ in zip(self.bijectors, split_y)]\n",
        "        x = tf.concat(split_x, axis=-1)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        split_x = (tf.split(x, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None\n",
        "                   else tf.split(x, self.block_sizes, axis=-1))\n",
        "        fldjs = [\n",
        "            b.forward_log_det_jacobian(x_, event_ndims=3)\n",
        "            for b, x_ in zip(self.bijectors, split_x)\n",
        "        ]\n",
        "        return sum(fldjs)\n",
        "\n",
        "    def _inverse_log_det_jacobian(self, y):\n",
        "        split_y = (tf.split(y, len(self.bijectors), axis=-1)\n",
        "                   if self.block_sizes is None\n",
        "                   else tf.split(y, self.block_sizes, axis=-1))\n",
        "        ildjs = [\n",
        "            b.inverse_log_det_jacobian(y_, event_ndims=3)\n",
        "            for b, y_ in zip(self.bijectors, split_y)\n",
        "        ]\n",
        "        return sum(ildjs)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06zwd69V4Szb",
        "outputId": "3fe09de6-422c-4387-c862-d50fe2c3ee39"
      },
      "source": [
        "#Inverse 1*1 Convolution\n",
        "def invertible_1x1_conv_LU(event_size, batch_shape=(), seed=None, dtype=tf.float32, name=\"lu_inv_conv\"):\n",
        "\n",
        "    with tf.name_scope(name or 'trainable_lu_factorisation'):\n",
        "        event_size = tf.convert_to_tensor(\n",
        "                        event_size, dtype=tf.int32, name='event_size')\n",
        "        batch_shape = tf.convert_to_tensor(\n",
        "                        batch_shape, dtype=event_size.dtype, name='batch_shape')\n",
        "        random_matrix = tf.Variable(tf.random.uniform(\n",
        "                        shape=tf.concat([batch_shape, [event_size, event_size]], axis=0),\n",
        "                        dtype=dtype,\n",
        "                        seed=seed),\n",
        "                        name='1x1_inv_conv_weights')\n",
        "        #QR decomposition gives us 2 matrix\n",
        "        # 0 index - orthogonal matrix which has orthornormal unit vector columns\n",
        "        # 1 index Right upper triangular matrix.\n",
        "        random_orthornormal = tf.linalg.qr(random_matrix).q\n",
        "\n",
        "        # we do LU decomposition of orthogonal matrix \n",
        "        # 0th index gives lower_upper triangular matrix\n",
        "        # 1 index give permutation matrix.\n",
        "        #lower_upper, permutation = tf.linalg.lu(random_orthornormal)\n",
        "\n",
        "        lower_upper = tf.Variable(tf.linalg.lu(random_orthornormal)[0], trainable=True, name='lower_upper')\n",
        "        permutation = tf.Variable(tf.linalg.lu(random_orthornormal)[1], trainable=False, name='permutation')\n",
        "\n",
        "        # lower_upper = tf.Variable(\n",
        "        #                 initial_value=lower_upper,\n",
        "        #                 trainable = True,\n",
        "        #                 name='lower_upper')\n",
        "        # permutation = tf.Variable(\n",
        "        #                 initial_value=permutation,\n",
        "        #                 trainable=False,\n",
        "        #                 name='permutation')\n",
        "        # print(\"lower upper: \", lower_upper)\n",
        "\n",
        "    # x = tf.random.uniform(shape=[2, 28, 28, 3], dtype=tf.float32)\n",
        "    # print(\"det jacobian\", inv_conv.inverse_log_det_jacobian(x,event_ndims=3)/inv_conv.inverse_log_det_jacobian(x,event_ndims=1))\n",
        "    return lower_upper, permutation\n",
        "\n",
        "def build_model(channels=3):\n",
        "    # conv1x1 setup\n",
        "    # t_lower_upper, t_permutation = invertible_1x1_conv_LU(channels)\n",
        "    lower_upper, permutation = invertible_1x1_conv_LU(channels)\n",
        "    inv_conv = tfb.MatvecLU(lower_upper, permutation, name='MatvecLU')\n",
        "    # tfb.MatvecLU(t_lower_upper, t_permutation, name='MatvecLU')\n",
        "    print('conv1x1 variable\\n', inv_conv.variables)\n",
        "    inv_conv1x1 = tfb.Invert(inv_conv)\n",
        "\n",
        "    # forward setup\n",
        "    fwd = tfp.layers.DistributionLambda(\n",
        "        lambda x: inv_conv(tfd.Deterministic(x)))\n",
        "    fwd.vars = inv_conv.trainable_variables\n",
        "\n",
        "    # inverse setup\n",
        "    inv = tfp.layers.DistributionLambda(\n",
        "        lambda x: inv_conv1x1(tfd.Deterministic(x)))\n",
        "    inv.vars = inv_conv1x1.trainable_variables\n",
        "    \n",
        "    x: tf.Tensor = tf.keras.Input(shape=[28, 28, channels])\n",
        "    fwd_x: tfp.distributions.TransformedDistribution = fwd(x)\n",
        "    rev_fwd_x: tfp.distributions.TransformedDistribution = inv(fwd_x)\n",
        "    example_model = tf.keras.Model(inputs=x, outputs=rev_fwd_x, name='conv1x1')\n",
        "    return example_model\n",
        "\n",
        "\n",
        "def test_conv1x1():\n",
        "    example_model = build_model()\n",
        "    example_model.trainable = True\n",
        "    example_model.summary()\n",
        "\n",
        "    real_x = tf.random.uniform(shape=[2, 28, 28, 3], dtype=tf.float32)\n",
        "    if example_model.weights == []:\n",
        "        print('No Trainable Variable exists')\n",
        "    else:\n",
        "        print('Some Trainable Variables exist')\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(real_x)\n",
        "        out_x = example_model(real_x)\n",
        "        out_x = out_x\n",
        "        loss = out_x - real_x\n",
        "    print(tf.math.reduce_sum(real_x - out_x))\n",
        "    print(example_model.predict(real_x).shape)\n",
        "    # print(\"log prob: \", example_model.log_prob(real_x))\n",
        "    # => nealy 0\n",
        "    # ex. tf.Tensor(1.3522818e-05, shape=(), dtype=float32)\n",
        "\n",
        "    try:\n",
        "        print(\"gradient: \",tape.gradient(loss, real_x).shape)\n",
        "    except Exception as e:\n",
        "        print('Cannot Calculate Gradient')\n",
        "        print(e)\n",
        "        \n",
        "test_conv1x1()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-6c980ad1df6a>:45: MatvecLU.__init__ (from tensorflow_probability.python.bijectors.scale_matvec_lu) is deprecated and will be removed after 2020-01-01.\n",
            "Instructions for updating:\n",
            "`MatvecLU` has been deprecated and renamed `ScaleMatvecLU`; please use that symbol instead.\n",
            "conv1x1 variable\n",
            " (<tf.Variable 'lu_inv_conv/lower_upper:0' shape=(3, 3) dtype=float32, numpy=\n",
            "array([[-0.7793306 ,  0.6101671 , -0.14261748],\n",
            "       [ 0.5125994 , -0.9719273 , -0.5640173 ],\n",
            "       [ 0.6194529 ,  0.8411388 ,  1.3202144 ]], dtype=float32)>, <tf.Variable 'lu_inv_conv/permutation:0' shape=(3,) dtype=int32, numpy=array([0, 1, 2], dtype=int32)>)\n",
            "Model: \"conv1x1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 3)]       0         \n",
            "_________________________________________________________________\n",
            "distribution_lambda (Distrib ((None, 28, 28, 3), (None 12        \n",
            "_________________________________________________________________\n",
            "distribution_lambda_1 (Distr ((None, 28, 28, 3), (None 12        \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 9\n",
            "Non-trainable params: 3\n",
            "_________________________________________________________________\n",
            "Some Trainable Variables exist\n",
            "tf.Tensor(3.0297459e-05, shape=(), dtype=float32)\n",
            "(2, 28, 28, 3)\n",
            "gradient:  (2, 28, 28, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMHgFcdv4eQh"
      },
      "source": [
        "#Squeeze layer\n",
        "class Squeeze(tfb.Bijector):\n",
        "    def __init__(self, factor=2, name='squeeze', forward_min_event_ndims=0, inverse_min_event_ndims=0, validate_args=False):\n",
        "        self._factor = factor\n",
        "        #self.name = name\n",
        "        super(Squeeze, self).__init__(name=name, is_constant_jacobian = True,\n",
        "                                      forward_min_event_ndims=forward_min_event_ndims,\n",
        "                                      inverse_min_event_ndims=inverse_min_event_ndims)\n",
        "        \n",
        "    @property\n",
        "    def factor(self):\n",
        "        return self._factor\n",
        "\n",
        "    def _forward(self, x):\n",
        "        (h,w,c) = x.shape[1:]\n",
        "        batch_size = tf.shape(x)[0:1]\n",
        "        temp_shape = tf.concat([batch_size, \n",
        "                                (h//self.factor, self.factor, w//self.factor, self.factor, c),\n",
        "                                ], axis=0)\n",
        "        output_shape = tf.concat([batch_size, \n",
        "                                (h//self.factor, w//self.factor, c*self.factor*self.factor),\n",
        "                                ], axis=0)\n",
        "        transpose_permutation = [0, 1, 3, 5, 2, 4]\n",
        "        y = tf.reshape(x, temp_shape)\n",
        "        y = tf.transpose(y, transpose_permutation)\n",
        "        y = tf.reshape(y, output_shape)\n",
        "        return y\n",
        "\n",
        "    def _inverse(self, y):\n",
        "        (h,w,c) = y.shape[1:]\n",
        "        batch_size = tf.shape(y)[0:1]\n",
        "        # temp_shape = (batch_size, h, w, c//(self.factor*self.factor), self.factor, self.factor)\n",
        "        temp_shape = tf.concat([batch_size, \n",
        "                                (h, w, c//(self.factor*self.factor), self.factor, self.factor),\n",
        "                                ], axis=0)\n",
        "        output_shape = tf.concat([batch_size, \n",
        "                                (h*self.factor, w*self.factor, c//(self.factor*self.factor)),\n",
        "                                ], axis=0)\n",
        "        transpose_permutation = [0, 1, 4, 2, 5, 3]\n",
        "        x = tf.reshape(y, temp_shape)\n",
        "        x = tf.transpose(x, transpose_permutation)\n",
        "        x = tf.reshape(x, output_shape)\n",
        "        return x\n",
        "\n",
        "    def _forward_log_det_jacobian(self, x):\n",
        "        return tf.constant(0.0)\n",
        "    \n",
        "    # def _inverse_log_det_jacobian(self, x, event_ndims=0):\n",
        "    #     return tf.constant(0.0)\n",
        "\n",
        "def test_squeeze():\n",
        "    factor = 2\n",
        "    x = tf.Variable([[[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 13, 14],\n",
        "                      [11, 12, 15, 16]]])\n",
        "    x = tf.expand_dims(x, axis=-1)\n",
        "    squeeze = Squeeze(factor=factor)\n",
        "    y = squeeze.forward(x)\n",
        "    z = squeeze.inverse(y)\n",
        "    print(tf.reduce_sum(x - z))\n",
        "\n",
        "    flow = tfd.TransformedDistribution(event_shape=[16, 16, 2],\n",
        "                                       distribution=tfd.Normal(loc=0.,\n",
        "                                                               scale=1.),\n",
        "                                       bijector=squeeze)\n",
        "    x = tf.random.normal([64, 16, 16, 2])\n",
        "    y = flow.bijector.forward(x)\n",
        "    log_prob = flow.log_prob(y)\n",
        "    print(x.shape, y.shape, log_prob.shape)\n",
        "    print(squeeze._forward_log_det_jacobian(x))\n",
        "    \n",
        "#test_squeeze()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejsI_ED-9dmK"
      },
      "source": [
        "def gen_flowSteps(\n",
        "    # for realnvp\n",
        "    input_shape: list,\n",
        "    n_hidden: list = [128, 128],\n",
        "    # for flowStep\n",
        "    k=4,\n",
        "    forward_min_event_ndims: int = 3,\n",
        "    validate_args: bool = False,\n",
        "    name: str = \"flow_step\",\n",
        "):\n",
        "    flow_step_list = []\n",
        "    for i in range(k):\n",
        "        t_lower_upper, t_permutation = invertible_1x1_conv_LU(input_shape[-1])\n",
        "        flow_step_list.append(ACT(input_shape[-1]))\n",
        "        flow_step_list.append(\n",
        "            tfb.MatvecLU(\n",
        "                t_lower_upper, t_permutation, name=\"{}_{}/matveclu\".format(name, i)\n",
        "            )\n",
        "        ),\n",
        "        flow_step_list.append(\n",
        "            ACL(\n",
        "                output_shape=input_shape,\n",
        "                n_hidden=n_hidden,\n",
        "                validate_args=validate_args,\n",
        "                name=\"{}_{}/acl\".format(name, i),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    flowSteps = tfb.Chain(\n",
        "        list(reversed(flow_step_list)), validate_args=validate_args, name=name\n",
        "    )\n",
        "    return flowSteps\n",
        "\n",
        "\n",
        "def test_gen_flowSteps():\n",
        "    flowSteps = gen_flowSteps(\n",
        "        k=2, input_shape=[16, 16, 4], forward_min_event_ndims=0, name=\"flowstep_0\"\n",
        "    )\n",
        "    x = tf.keras.Input([16, 16, 4])\n",
        "    y = flowSteps(x)\n",
        "    # tf.keras.Model(x, y).summary()\n",
        "\n",
        "    x = tf.random.normal([6, 16, 16, 4])\n",
        "    y = flowSteps.forward(x)\n",
        "    z = flowSteps.inverse(y)\n",
        "    return tf.reduce_sum(z - x)\n",
        "\n",
        "\n",
        "#test_gen_flowSteps()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhMTogvJ8HZl",
        "outputId": "05764d14-9cc1-41ba-f80e-5a201d50e5b0"
      },
      "source": [
        "def gen_flow(input_shape, level=3, flow_step_args: dict = None):\n",
        "    def _gen_input_shapes(input_shape, level):\n",
        "        input_shape = input_shape\n",
        "        input_shapes = []\n",
        "        for i in range(level):\n",
        "            input_shape = [\n",
        "                input_shape[0] // 2,\n",
        "                input_shape[1] // 2,\n",
        "                input_shape[2] * 2,\n",
        "            ]\n",
        "            input_shapes.append(input_shape)\n",
        "        return input_shapes\n",
        "\n",
        "    input_shape[-1] = input_shape[-1] * 2\n",
        "    input_shapes = _gen_input_shapes(input_shape, level)\n",
        "\n",
        "    def _add_flow(_input_shapes, flow_step_args):\n",
        "        flow_lists = []\n",
        "        flow_lists.append(\n",
        "            Squeeze(name=\"Squeeze_{}\".format(level - len(_input_shapes)))\n",
        "        )\n",
        "        flowSteps = gen_flowSteps(\n",
        "           k=2,\n",
        "           input_shape=_input_shapes[0],\n",
        "           name=\"Flowsteps_{}\".format(level - len(_input_shapes)),\n",
        "        )\n",
        "        flow_lists.append(flowSteps)\n",
        "        if len(_input_shapes) != 1:\n",
        "            flow_lists.append(\n",
        "                Blockwise(\n",
        "                    [\n",
        "                        tfb.Identity(),\n",
        "                        tfb.Chain(\n",
        "                            list(reversed(_add_flow(_input_shapes[1:], flow_step_args)))\n",
        "                        ),\n",
        "                    ],\n",
        "                    name=\"Blockwise3D_{}\".format(level - len(_input_shapes)),\n",
        "                )\n",
        "            )\n",
        "        flow_lists.append(\n",
        "            tfb.Invert(\n",
        "                Squeeze(name=\"Unsqueeze_{}\".format(level - len(_input_shapes)))\n",
        "            )\n",
        "        )\n",
        "        return flow_lists\n",
        "\n",
        "    return tfb.Chain(list(reversed(_add_flow(input_shapes, level))))\n",
        "\n",
        "\n",
        "def test_gen_flow():\n",
        "    flow = gen_flow([32, 32, 1])\n",
        "    print(len(flow.trainable_variables))\n",
        "    x = tf.keras.Input([32, 32, 1])\n",
        "    y = flow.forward(x)\n",
        "    # tf.keras.Model(x, y).summary()\n",
        "    tf.keras.utils.plot_model(\n",
        "        tf.keras.Model(x, y), show_shapes=True, to_file=\"realnvp.png\"\n",
        "    )\n",
        "    x = tf.random.normal([3, 32, 32, 1])\n",
        "    y = flow.forward(x)\n",
        "    z = flow.inverse(y) \n",
        "    return tf.reduce_sum(z - x)\n",
        "\n",
        "test_gen_flow()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azep_jc14obT"
      },
      "source": [
        "**Build Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK3NN74f4sac"
      },
      "source": [
        "# class Flow(tfb.Bijector):\n",
        "#     def __init__(self, L,\n",
        "#         K,\n",
        "#         activation,\n",
        "#         input_shape,\n",
        "#         num_of_hidden,\n",
        "#         name=\"flow_model\"\n",
        "#     ):\n",
        "\n",
        "#         super(Flow, self).__init__(\n",
        "#             validate_args=False,\n",
        "#             forward_min_event_ndims=0,\n",
        "#             name=name)\n",
        "        \n",
        "#         self.L = L\n",
        "#         self.K = K\n",
        "#         self.input_shapes = self._get_input_shapes(input_shape, L)\n",
        "\n",
        "#         #print(self.input_shapes)\n",
        "#         self.flow_network = tfb.Chain(list(reversed(self.add_flow(self.input_shapes, name=name+\"/Flow_0\"))))\n",
        "\n",
        "#     def add_flow(self, input_shape, name):\n",
        "#         flow_layers = []\n",
        "#         squeeze_layer = Squeeze(2, name=name+\"/squeeze_{}\".format(self.L - len(input_shape)))\n",
        "#         flow_layers.append(squeeze_layer)\n",
        "#         flow_steps = self._flow_steps(self.K, input_shape=input_shape[0], name=name+\"/flow_step_{}\".format(self.L - len(input_shape)))\n",
        "#         flow_layers.append(flow_steps)\n",
        "#         if len(input_shape) != 1:\n",
        "#             #Blockwise\n",
        "#             blockwise_mod = Blockwise(bijectors=[tfb.Identity(), tfb.Chain(\n",
        "#                             list(reversed(self.add_flow(input_shape[1:], name = name.split('_')[0]+\"_{}\".format(self.L-len(input_shape[1:])))))\n",
        "#                         )], name=name+\"/blockwise_{}\".format(self.L - len(input_shape)))\n",
        "#             flow_layers.append(blockwise_mod)\n",
        "#         squeeze_invert = tfb.Invert(Squeeze(2, name=name+\"/unsqueeze_{}\".format(self.L - len(input_shape))))\n",
        "#         flow_layers.append(squeeze_invert)\n",
        "#         return flow_layers\n",
        "    \n",
        "#     def _flow_steps(self, K, input_shape, name):\n",
        "#         flow_steps_list = []\n",
        "#         for i in range(K):\n",
        "#             act_norm = ACT(input_shape[-1], False, name=name+\"/actnorm_{}\".format(i))\n",
        "#             inv_conv = invertible_1x1_conv_LU(input_shape[-1], name=name+\"/inv_conv_1x1_{}\".format(i))\n",
        "#             acl = ACL(input_shape, name=name+\"/acl_{}\".format(i))\n",
        "            \n",
        "#             flow_steps_list.append(act_norm)\n",
        "#             flow_steps_list.append(inv_conv)\n",
        "#             flow_steps_list.append(acl)\n",
        "\n",
        "#         flow_steps = tfb.Chain(list(reversed(flow_steps_list)), validate_args=False, name=name)\n",
        "#         return flow_steps\n",
        "    \n",
        "#     def _get_input_shapes(self, input_shape, level):\n",
        "#         # print(level)\n",
        "#         input_shapes = []\n",
        "#         for i in range(level):\n",
        "#             input_shape = [input_shape[0]//2,\n",
        "#                             input_shape[1]//2,\n",
        "#                             (input_shape[2] * 4) - (i * 24)]\n",
        "#             input_shapes.append(input_shape)\n",
        "\n",
        "#         return input_shapes\n",
        "#     # model paramters\n",
        "    \n",
        "# class Glow_Model(tf.keras.Model):\n",
        "#     def __init__(self, name):\n",
        "#         L=3\n",
        "#         K=32\n",
        "#         activation = \"relu\"\n",
        "#         input_shape = [32,32,3]\n",
        "#         num_of_hidden = [512,512]\n",
        "#         name=\"glow_model\"\n",
        "#         print(\"inside glow model init\")\n",
        "#         super(Glow_Model, self).__init__( name)\n",
        "\n",
        "#         self.flow_model = Flow(L, K, activation, input_shape, num_of_hidden, name=name+\"/flow_bijector\")\n",
        "\n",
        "#         self.flow_distribution_obj = tfd.TransformedDistribution(\n",
        "#                                 event_shape=input_shape,\n",
        "#                                 distribution=tfd.Normal(loc=0.0, scale=1.0),\n",
        "#                                 bijector=self.flow_model                        \n",
        "#                             )\n",
        "        \n",
        "#     def call(self, inputs):\n",
        "#         return self.flow_distribution_obj.bijector.forward(inputs)\n",
        "\n",
        "#     def log_prob(self, inputs):\n",
        "#         return self.flow_distribution_obj.log_prob(inputs)\n",
        "    \n",
        "#     def getFlowSample(self, num):\n",
        "#         return self.flow_distribution_obj.sample(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOZiJb6g8bXT",
        "outputId": "5cdd5fca-baa1-4fe6-a39e-94104f5647d1"
      },
      "source": [
        "flow_bijector = gen_flow([32, 32, 3], level=3)\n",
        "print(len(flow_bijector.trainable_variables))\n",
        "flow = tfd.TransformedDistribution(\n",
        "    event_shape =[32, 32, 3],\n",
        "    distribution=tfd.Normal(loc=0.0, scale=1.0),\n",
        "    bijector=flow_bijector\n",
        ")\n",
        "print('trainable_variables: ', len(flow.bijector.trainable_variables))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66\n",
            "trainable_variables:  66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8pJJ11Q447c"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u5LnKuxI48UI",
        "outputId": "a238f366-1ffe-44a3-9669-decc22be02f6"
      },
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from keras import backend as KB\n",
        "\n",
        "L=3\n",
        "K=32\n",
        "epochs = 50\n",
        "learning_rate = 1e-4\n",
        "BATCH_SIZE=128\n",
        "input_shape = [32,32,3]\n",
        "\n",
        "bpd_factor = np.log(2) * input_shape[0] * input_shape[1] * input_shape[2]\n",
        "\n",
        "# glow_model = Glow_Model(name=\"glow_model\")\n",
        "# glow_model.build((1,32,32,3))\n",
        "# print(glow_model.summary())\n",
        "#flow = Flow(L, K, \"relu\", [32,32,3], [512,512], name=\"test\")\n",
        "#flow_distribution = tfd.TransformedDistribution(\n",
        "#                            event_shape=input_shape,\n",
        "#                            distribution=tfd.Normal(loc=0.0, scale=1.0),\n",
        "#                            bijector=flow.flow_network                       \n",
        "#                        )\n",
        "# x = tf.random.normal([3, 32, 32, 3])\n",
        "# y = flow_distribution.bijector.forward(x)\n",
        "# z = flow_distribution.bijector.inverse(y)\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.add_subplot(1,2,1)\n",
        "# plt.imshow(x[0])\n",
        "# # plt.show()\n",
        "# fig.add_subplot(1,2,2)\n",
        "# plt.imshow(z[0])\n",
        "# # plt.show()\n",
        "# print(tf.reduce_sum(z-x))\n",
        "# print(flow_distribution.log_prob(x).shape)\n",
        "\n",
        "# @tf.function\n",
        "def loss():\n",
        "    #x_ = np.clip(np.floor(x), 0, 255) /255.0\n",
        "    #print(x.shape)\n",
        "    # x_ = (np.clip(np.floor(x), 0, 255)/(255.0/2.0)) - 1\n",
        "    #x_.astype(np.float32)\n",
        "    # print(\"input x: \", np.around(x_[0], 3))\n",
        "    #plt.imshow(x[0])\n",
        "    #plt.show()\n",
        "    # print(np.max(x_[0,:,:,:]))\n",
        "    # print(\"bpd factor: \", bpd_factor, x_.shape)\n",
        "    log_det = - tf.reduce_mean(flow.log_prob(x))\n",
        "    #print(\"log det: \", log_det)\n",
        "    if log_det.numpy() == np.nan:\n",
        "        return log_det\n",
        "    else:\n",
        "        return log_det / bpd_factor\n",
        "    # return tf.constant(1234)\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate) \n",
        "log = tf.summary.create_file_writer('checkpoints')\n",
        "avg_loss = tf.keras.metrics.Mean(name='loss', dtype=tf.float32)\n",
        "dataset='mnist'\n",
        "# train_iterator, test_iterator = get_data(dataset, BATCH_SIZE)\n",
        "train_iterator, test_iterator = get_data_alt(1000, BATCH_SIZE)\n",
        "# train_its = int(60000/BATCH_SIZE)\n",
        "\n",
        "#checkpointing \n",
        "checkpoint_path=Path('/content/drive/MyDrive/glow/checkpoints/checkpoint_mnist')\n",
        "ckpt = tf.train.Checkpoint(model=flow, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
        "                                            checkpoint_path,\n",
        "                                            max_to_keep=3)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('[Flow] Latest checkpoint restored!!')\n",
        "\n",
        "KB.set_value(optimizer.learning_rate, 1e-3)\n",
        "\n",
        "flag = False\n",
        "\n",
        "for e in range(epochs):\n",
        "    if flag:\n",
        "        break\n",
        "    # for i in range(train_its):\n",
        "    for i in train_iterator:\n",
        "        x = i['img']\n",
        "        # x, y = train_iterator()\n",
        "        # print(x.shape)\n",
        "        with tf.GradientTape() as tape:\n",
        "            log_prob_loss = loss()\n",
        "            # print('log prob loss : ', log_prob_loss)\n",
        "        \n",
        "        # for f_var in flow.trainable_variables:\n",
        "        #     print('This is flow train vars : ', f_var.name)\n",
        "        grads = tape.gradient(log_prob_loss, flow.trainable_variables)\n",
        "        # print('This is grad value :', grads)\n",
        "        # break\n",
        "        optimizer.apply_gradients(zip(grads, flow.trainable_variables))\n",
        "        # print(\"returned log prob loss: \", log_prob_loss)\n",
        "        if tf.math.is_nan(log_prob_loss):\n",
        "            flag = True\n",
        "            print(\"loss is nan\")\n",
        "            break\n",
        "        avg_loss.update_state(log_prob_loss)\n",
        "        \n",
        "        if tf.equal(optimizer.iterations % 100, 0):\n",
        "          print(\"Epoch {} Step {} Loss {:.6f}\".format(e, optimizer.iterations.numpy(), avg_loss.result()))\n",
        "\n",
        "        if tf.equal(optimizer.iterations % 100, 0):\n",
        "            with log.as_default():\n",
        "                tf.summary.scalar(\"loss\", avg_loss.result(), step=optimizer.iterations)\n",
        "                avg_loss.reset_states()\n",
        "    \n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print('Saving checkpoint for epoch {} at {}'.format(\n",
        "                    e + 1, ckpt_save_path))\n",
        "# print(train_iterator())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Flow] Latest checkpoint restored!!\n",
            "Epoch 0 Step 77400 Loss 705.638855\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-774\n",
            "Epoch 0 Step 77500 Loss 679.874451\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-775\n",
            "Epoch 0 Step 77600 Loss 658.481323\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-776\n",
            "Epoch 0 Step 77700 Loss 637.424011\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-777\n",
            "Epoch 1 Step 77800 Loss 617.304504\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-778\n",
            "Epoch 1 Step 77900 Loss 601.019348\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-779\n",
            "Epoch 1 Step 78000 Loss 586.898682\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-780\n",
            "Epoch 1 Step 78100 Loss 574.720703\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-781\n",
            "Epoch 1 Step 78200 Loss 562.018005\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-782\n",
            "Epoch 2 Step 78300 Loss 549.233948\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-783\n",
            "Epoch 2 Step 78400 Loss 544.818542\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-784\n",
            "Epoch 2 Step 78500 Loss 530.123230\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-785\n",
            "Epoch 2 Step 78600 Loss 525.633240\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-786\n",
            "Epoch 2 Step 78700 Loss 528.556152\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-787\n",
            "Epoch 3 Step 78800 Loss 499.059265\n",
            "Saving checkpoint for epoch 4 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-788\n",
            "Epoch 3 Step 78900 Loss 505.868134\n",
            "Saving checkpoint for epoch 4 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-789\n",
            "Epoch 3 Step 79000 Loss 505.143127\n",
            "Saving checkpoint for epoch 4 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-790\n",
            "Epoch 3 Step 79100 Loss 495.144653\n",
            "Saving checkpoint for epoch 4 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-791\n",
            "Epoch 4 Step 79200 Loss 476.839264\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-792\n",
            "Epoch 4 Step 79300 Loss 479.628357\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-793\n",
            "Epoch 4 Step 79400 Loss 487.654694\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-794\n",
            "Epoch 4 Step 79500 Loss 465.692902\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-795\n",
            "Epoch 4 Step 79600 Loss 465.313995\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-796\n",
            "Epoch 5 Step 79700 Loss 452.864807\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-797\n",
            "Epoch 5 Step 79800 Loss 441.241028\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-798\n",
            "Epoch 5 Step 79900 Loss 445.131012\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-799\n",
            "Epoch 5 Step 80000 Loss 435.258972\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-800\n",
            "Epoch 5 Step 80100 Loss 441.367767\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-801\n",
            "Epoch 6 Step 80200 Loss 423.070007\n",
            "Saving checkpoint for epoch 7 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-802\n",
            "Epoch 6 Step 80300 Loss 412.467896\n",
            "Saving checkpoint for epoch 7 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-803\n",
            "Epoch 6 Step 80400 Loss 419.868469\n",
            "Saving checkpoint for epoch 7 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-804\n",
            "Epoch 6 Step 80500 Loss 414.162659\n",
            "Saving checkpoint for epoch 7 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-805\n",
            "Epoch 7 Step 80600 Loss 416.360352\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-806\n",
            "Epoch 7 Step 80700 Loss 389.292572\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-807\n",
            "Epoch 7 Step 80800 Loss 405.815277\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-808\n",
            "Epoch 7 Step 80900 Loss 386.656067\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-809\n",
            "Epoch 7 Step 81000 Loss 387.752960\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-810\n",
            "Epoch 8 Step 81100 Loss 370.400757\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-811\n",
            "Epoch 8 Step 81200 Loss 372.582916\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-812\n",
            "Epoch 8 Step 81300 Loss 384.945465\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-813\n",
            "Epoch 8 Step 81400 Loss 363.098785\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-814\n",
            "Epoch 8 Step 81500 Loss 366.136688\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-815\n",
            "Epoch 9 Step 81600 Loss 354.440399\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-816\n",
            "Epoch 9 Step 81700 Loss 356.858521\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-817\n",
            "Epoch 9 Step 81800 Loss 422.750427\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-818\n",
            "Epoch 9 Step 81900 Loss 349.660156\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-819\n",
            "Epoch 10 Step 82000 Loss 321.090851\n",
            "Saving checkpoint for epoch 11 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-820\n",
            "Epoch 10 Step 82100 Loss 336.181915\n",
            "Saving checkpoint for epoch 11 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-821\n",
            "Epoch 10 Step 82200 Loss 319.493225\n",
            "Saving checkpoint for epoch 11 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-822\n",
            "Epoch 10 Step 82300 Loss 334.759064\n",
            "Saving checkpoint for epoch 11 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-823\n",
            "Epoch 10 Step 82400 Loss 330.067230\n",
            "Saving checkpoint for epoch 11 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-824\n",
            "Epoch 11 Step 82500 Loss 320.810974\n",
            "Saving checkpoint for epoch 12 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-825\n",
            "Epoch 11 Step 82600 Loss 317.645874\n",
            "Saving checkpoint for epoch 12 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-826\n",
            "Epoch 11 Step 82700 Loss 314.522400\n",
            "Saving checkpoint for epoch 12 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-827\n",
            "Epoch 11 Step 82800 Loss 299.580627\n",
            "Saving checkpoint for epoch 12 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-828\n",
            "Epoch 11 Step 82900 Loss 313.743713\n",
            "Saving checkpoint for epoch 12 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-829\n",
            "Epoch 12 Step 83000 Loss 325.394501\n",
            "Saving checkpoint for epoch 13 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-830\n",
            "Epoch 12 Step 83100 Loss 355.228149\n",
            "Saving checkpoint for epoch 13 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-831\n",
            "Epoch 12 Step 83200 Loss 274.994843\n",
            "Saving checkpoint for epoch 13 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-832\n",
            "Epoch 12 Step 83300 Loss 283.738312\n",
            "Saving checkpoint for epoch 13 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-833\n",
            "Epoch 13 Step 83400 Loss 277.949615\n",
            "Saving checkpoint for epoch 14 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-834\n",
            "Epoch 13 Step 83500 Loss 275.027283\n",
            "Saving checkpoint for epoch 14 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-835\n",
            "Epoch 13 Step 83600 Loss 292.101776\n",
            "Saving checkpoint for epoch 14 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-836\n",
            "Epoch 13 Step 83700 Loss 268.579193\n",
            "Saving checkpoint for epoch 14 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-837\n",
            "Epoch 13 Step 83800 Loss 6808.884277\n",
            "Saving checkpoint for epoch 14 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-838\n",
            "Epoch 14 Step 83900 Loss 3116.603760\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-839\n",
            "Epoch 14 Step 84000 Loss 637.954590\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-840\n",
            "Epoch 14 Step 84200 Loss 457.872650\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-842\n",
            "Epoch 14 Step 84300 Loss 429.038086\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-843\n",
            "Epoch 15 Step 84400 Loss 406.218628\n",
            "Saving checkpoint for epoch 16 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-844\n",
            "Epoch 15 Step 84500 Loss 391.170868\n",
            "Saving checkpoint for epoch 16 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-845\n",
            "Epoch 15 Step 84600 Loss 377.318115\n",
            "Saving checkpoint for epoch 16 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-846\n",
            "Epoch 15 Step 84700 Loss 364.629486\n",
            "Saving checkpoint for epoch 16 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-847\n",
            "Epoch 15 Step 84800 Loss 352.389252\n",
            "Saving checkpoint for epoch 16 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-848\n",
            "Epoch 16 Step 84900 Loss 340.420990\n",
            "Saving checkpoint for epoch 17 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-849\n",
            "Epoch 16 Step 85000 Loss 331.360535\n",
            "Saving checkpoint for epoch 17 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-850\n",
            "Epoch 16 Step 85100 Loss 322.453522\n",
            "Saving checkpoint for epoch 17 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-851\n",
            "Epoch 16 Step 85200 Loss 314.977570\n",
            "Saving checkpoint for epoch 17 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-852\n",
            "Epoch 17 Step 85300 Loss 308.676239\n",
            "Saving checkpoint for epoch 18 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-853\n",
            "Epoch 17 Step 85400 Loss 302.743683\n",
            "Saving checkpoint for epoch 18 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-854\n",
            "Epoch 17 Step 85500 Loss 298.537201\n",
            "Saving checkpoint for epoch 18 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-855\n",
            "Epoch 17 Step 85600 Loss 293.393463\n",
            "Saving checkpoint for epoch 18 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-856\n",
            "Epoch 17 Step 85700 Loss 288.721313\n",
            "Saving checkpoint for epoch 18 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-857\n",
            "Epoch 18 Step 85800 Loss 284.031616\n",
            "Saving checkpoint for epoch 19 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-858\n",
            "Epoch 18 Step 85900 Loss 280.496582\n",
            "Saving checkpoint for epoch 19 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-859\n",
            "Epoch 18 Step 86000 Loss 277.268768\n",
            "Saving checkpoint for epoch 19 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-860\n",
            "Epoch 18 Step 86100 Loss 273.671600\n",
            "Saving checkpoint for epoch 19 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-861\n",
            "Epoch 18 Step 86200 Loss 270.478363\n",
            "Saving checkpoint for epoch 19 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-862\n",
            "Epoch 19 Step 86300 Loss 266.211609\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-863\n",
            "Epoch 19 Step 86400 Loss 264.581482\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-864\n",
            "Epoch 19 Step 86500 Loss 262.873352\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-865\n",
            "Epoch 19 Step 86600 Loss 257.947144\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-866\n",
            "Epoch 20 Step 86700 Loss 260.912994\n",
            "Saving checkpoint for epoch 21 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-867\n",
            "Epoch 20 Step 86800 Loss 252.941879\n",
            "Saving checkpoint for epoch 21 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-868\n",
            "Epoch 20 Step 86900 Loss 252.228729\n",
            "Saving checkpoint for epoch 21 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-869\n",
            "Epoch 20 Step 87000 Loss 251.981842\n",
            "Saving checkpoint for epoch 21 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-870\n",
            "Epoch 20 Step 87100 Loss 249.959061\n",
            "Saving checkpoint for epoch 21 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-871\n",
            "Epoch 21 Step 87200 Loss 244.744827\n",
            "Saving checkpoint for epoch 22 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-872\n",
            "Epoch 21 Step 87300 Loss 249.915039\n",
            "Saving checkpoint for epoch 22 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-873\n",
            "Epoch 21 Step 87400 Loss 239.229675\n",
            "Saving checkpoint for epoch 22 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-874\n",
            "Epoch 21 Step 87500 Loss 240.663651\n",
            "Saving checkpoint for epoch 22 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-875\n",
            "Epoch 21 Step 87600 Loss 237.378677\n",
            "Saving checkpoint for epoch 22 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-876\n",
            "Epoch 22 Step 87700 Loss 236.173569\n",
            "Saving checkpoint for epoch 23 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-877\n",
            "Epoch 22 Step 87800 Loss 232.865311\n",
            "Saving checkpoint for epoch 23 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-878\n",
            "Epoch 22 Step 87900 Loss 231.041504\n",
            "Saving checkpoint for epoch 23 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-879\n",
            "Epoch 22 Step 88000 Loss 227.483337\n",
            "Saving checkpoint for epoch 23 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-880\n",
            "Epoch 23 Step 88100 Loss 228.133804\n",
            "Saving checkpoint for epoch 24 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-881\n",
            "Epoch 23 Step 88200 Loss 234.838470\n",
            "Saving checkpoint for epoch 24 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-882\n",
            "Epoch 23 Step 88300 Loss 211.687943\n",
            "Saving checkpoint for epoch 24 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-883\n",
            "Epoch 23 Step 88400 Loss 225.801804\n",
            "Saving checkpoint for epoch 24 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-884\n",
            "Epoch 23 Step 88500 Loss 209.813263\n",
            "Saving checkpoint for epoch 24 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-885\n",
            "Epoch 24 Step 88600 Loss 212.786011\n",
            "Saving checkpoint for epoch 25 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-886\n",
            "Epoch 24 Step 88700 Loss 213.262360\n",
            "Saving checkpoint for epoch 25 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-887\n",
            "Epoch 24 Step 88800 Loss 206.453201\n",
            "Saving checkpoint for epoch 25 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-888\n",
            "Epoch 24 Step 88900 Loss 205.035568\n",
            "Saving checkpoint for epoch 25 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-889\n",
            "Epoch 24 Step 89000 Loss 197.090042\n",
            "Saving checkpoint for epoch 25 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-890\n",
            "Epoch 25 Step 89100 Loss 192.694321\n",
            "Saving checkpoint for epoch 26 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-891\n",
            "Epoch 25 Step 89200 Loss 190.554733\n",
            "Saving checkpoint for epoch 26 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-892\n",
            "Epoch 25 Step 89300 Loss 184.748245\n",
            "Saving checkpoint for epoch 26 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-893\n",
            "Epoch 25 Step 89400 Loss 195.466934\n",
            "Saving checkpoint for epoch 26 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-894\n",
            "Epoch 26 Step 89500 Loss 176.678589\n",
            "Saving checkpoint for epoch 27 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-895\n",
            "Epoch 26 Step 89600 Loss 180.847046\n",
            "Saving checkpoint for epoch 27 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-896\n",
            "Epoch 26 Step 89700 Loss 172.964172\n",
            "Saving checkpoint for epoch 27 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-897\n",
            "Epoch 26 Step 89800 Loss 169.609558\n",
            "Saving checkpoint for epoch 27 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-898\n",
            "Epoch 26 Step 89900 Loss 169.640289\n",
            "Saving checkpoint for epoch 27 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-899\n",
            "Epoch 27 Step 90000 Loss 164.650604\n",
            "Saving checkpoint for epoch 28 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-900\n",
            "Epoch 27 Step 90100 Loss 155.450974\n",
            "Saving checkpoint for epoch 28 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-901\n",
            "Epoch 27 Step 90200 Loss 153.123825\n",
            "Saving checkpoint for epoch 28 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-902\n",
            "Epoch 27 Step 90300 Loss 149.539993\n",
            "Saving checkpoint for epoch 28 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-903\n",
            "Epoch 27 Step 90400 Loss 143.775681\n",
            "Saving checkpoint for epoch 28 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-904\n",
            "Epoch 28 Step 90500 Loss 142.500366\n",
            "Saving checkpoint for epoch 29 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-905\n",
            "Epoch 28 Step 90600 Loss 133.458298\n",
            "Saving checkpoint for epoch 29 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-906\n",
            "Epoch 28 Step 90700 Loss 137.057175\n",
            "Saving checkpoint for epoch 29 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-907\n",
            "Epoch 28 Step 90800 Loss 123.891983\n",
            "Saving checkpoint for epoch 29 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-908\n",
            "Epoch 28 Step 90900 Loss 127.164818\n",
            "Saving checkpoint for epoch 29 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-909\n",
            "Epoch 29 Step 91000 Loss 123.570267\n",
            "Saving checkpoint for epoch 30 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-910\n",
            "Epoch 29 Step 91100 Loss 131.293228\n",
            "Saving checkpoint for epoch 30 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-911\n",
            "Epoch 29 Step 91200 Loss 111.090378\n",
            "Saving checkpoint for epoch 30 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-912\n",
            "Epoch 29 Step 91300 Loss 117.454239\n",
            "Saving checkpoint for epoch 30 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-913\n",
            "Epoch 30 Step 91400 Loss 119.284874\n",
            "Saving checkpoint for epoch 31 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-914\n",
            "Epoch 30 Step 91500 Loss 111.629280\n",
            "Saving checkpoint for epoch 31 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-915\n",
            "Epoch 30 Step 91600 Loss 102.016731\n",
            "Saving checkpoint for epoch 31 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-916\n",
            "Epoch 30 Step 91700 Loss 99.071754\n",
            "Saving checkpoint for epoch 31 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-917\n",
            "Epoch 30 Step 91800 Loss 99.517540\n",
            "Saving checkpoint for epoch 31 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-918\n",
            "Epoch 31 Step 91900 Loss 103.519081\n",
            "Saving checkpoint for epoch 32 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-919\n",
            "Epoch 31 Step 92000 Loss 94.715919\n",
            "Saving checkpoint for epoch 32 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-920\n",
            "Epoch 31 Step 92100 Loss 93.704987\n",
            "Saving checkpoint for epoch 32 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-921\n",
            "Epoch 31 Step 92200 Loss 90.197899\n",
            "Saving checkpoint for epoch 32 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-922\n",
            "Epoch 31 Step 92300 Loss 86.516449\n",
            "Saving checkpoint for epoch 32 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-923\n",
            "Epoch 32 Step 92400 Loss 86.088135\n",
            "Saving checkpoint for epoch 33 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-924\n",
            "Epoch 32 Step 92500 Loss 79.323288\n",
            "Saving checkpoint for epoch 33 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-925\n",
            "Epoch 32 Step 92600 Loss 86.029305\n",
            "Saving checkpoint for epoch 33 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-926\n",
            "Epoch 32 Step 92700 Loss 71.338387\n",
            "Saving checkpoint for epoch 33 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-927\n",
            "Epoch 33 Step 92800 Loss 73.885284\n",
            "Saving checkpoint for epoch 34 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-928\n",
            "Epoch 33 Step 92900 Loss 72.737190\n",
            "Saving checkpoint for epoch 34 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-929\n",
            "Epoch 33 Step 93000 Loss 78.653763\n",
            "Saving checkpoint for epoch 34 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-930\n",
            "Epoch 33 Step 93100 Loss 66.994522\n",
            "Saving checkpoint for epoch 34 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-931\n",
            "Epoch 33 Step 93200 Loss 64.912598\n",
            "Saving checkpoint for epoch 34 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-932\n",
            "Epoch 34 Step 93300 Loss 82.358063\n",
            "Saving checkpoint for epoch 35 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-933\n",
            "Epoch 34 Step 93400 Loss 61.193672\n",
            "Saving checkpoint for epoch 35 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-934\n",
            "Epoch 34 Step 93500 Loss 57.343449\n",
            "Saving checkpoint for epoch 35 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-935\n",
            "Epoch 34 Step 93600 Loss 65.444756\n",
            "Saving checkpoint for epoch 35 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-936\n",
            "Epoch 34 Step 93700 Loss 54.211723\n",
            "Saving checkpoint for epoch 35 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-937\n",
            "Epoch 35 Step 93800 Loss 55.979733\n",
            "Saving checkpoint for epoch 36 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-938\n",
            "Epoch 35 Step 93900 Loss 57.747360\n",
            "Saving checkpoint for epoch 36 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-939\n",
            "Epoch 35 Step 94000 Loss 52.708897\n",
            "Saving checkpoint for epoch 36 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-940\n",
            "Epoch 35 Step 94100 Loss 51.187469\n",
            "Saving checkpoint for epoch 36 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-941\n",
            "Epoch 36 Step 94200 Loss 54.963673\n",
            "Saving checkpoint for epoch 37 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-942\n",
            "Epoch 36 Step 94300 Loss 46.000454\n",
            "Saving checkpoint for epoch 37 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-943\n",
            "Epoch 36 Step 94400 Loss 46.208847\n",
            "Saving checkpoint for epoch 37 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-944\n",
            "Epoch 36 Step 94500 Loss 84.905754\n",
            "Saving checkpoint for epoch 37 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-945\n",
            "Epoch 36 Step 94600 Loss 46.668182\n",
            "Saving checkpoint for epoch 37 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-946\n",
            "Epoch 37 Step 94700 Loss 35.144360\n",
            "Saving checkpoint for epoch 38 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-947\n",
            "Epoch 37 Step 94800 Loss 44.534298\n",
            "Saving checkpoint for epoch 38 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-948\n",
            "Epoch 37 Step 94900 Loss 39.514980\n",
            "Saving checkpoint for epoch 38 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-949\n",
            "Epoch 37 Step 95000 Loss 38.294807\n",
            "Saving checkpoint for epoch 38 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-950\n",
            "Epoch 37 Step 95100 Loss 43.967358\n",
            "Saving checkpoint for epoch 38 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-951\n",
            "Epoch 38 Step 95200 Loss 38.479427\n",
            "Saving checkpoint for epoch 39 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-952\n",
            "Epoch 38 Step 95300 Loss 35.962250\n",
            "Saving checkpoint for epoch 39 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-953\n",
            "Epoch 38 Step 95400 Loss 34.871387\n",
            "Saving checkpoint for epoch 39 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-954\n",
            "Epoch 38 Step 95500 Loss 34.009701\n",
            "Saving checkpoint for epoch 39 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-955\n",
            "Epoch 39 Step 95600 Loss 10103.206055\n",
            "Saving checkpoint for epoch 40 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-956\n",
            "Epoch 39 Step 95700 Loss 1429.357300\n",
            "Saving checkpoint for epoch 40 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-957\n",
            "Epoch 39 Step 95800 Loss 447.905029\n",
            "Saving checkpoint for epoch 40 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-958\n",
            "Epoch 39 Step 95900 Loss 312.506775\n",
            "Saving checkpoint for epoch 40 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-959\n",
            "Epoch 39 Step 96000 Loss 273.584869\n",
            "Saving checkpoint for epoch 40 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-960\n",
            "Epoch 40 Step 96100 Loss 249.158676\n",
            "Saving checkpoint for epoch 41 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-961\n",
            "Epoch 40 Step 96200 Loss 232.054413\n",
            "Saving checkpoint for epoch 41 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-962\n",
            "Epoch 40 Step 96300 Loss 219.223022\n",
            "Saving checkpoint for epoch 41 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-963\n",
            "Epoch 40 Step 96400 Loss 205.592102\n",
            "Saving checkpoint for epoch 41 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-964\n",
            "Epoch 40 Step 96500 Loss 195.046341\n",
            "Saving checkpoint for epoch 41 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-965\n",
            "Epoch 41 Step 96600 Loss 184.602661\n",
            "Saving checkpoint for epoch 42 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-966\n",
            "Epoch 41 Step 96700 Loss 175.500229\n",
            "Saving checkpoint for epoch 42 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-967\n",
            "Epoch 41 Step 96800 Loss 166.968552\n",
            "Saving checkpoint for epoch 42 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-968\n",
            "Epoch 41 Step 96900 Loss 159.579239\n",
            "Saving checkpoint for epoch 42 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-969\n",
            "Epoch 42 Step 97000 Loss 152.737762\n",
            "Saving checkpoint for epoch 43 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-970\n",
            "Epoch 42 Step 97100 Loss 144.776062\n",
            "Saving checkpoint for epoch 43 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-971\n",
            "Epoch 42 Step 97200 Loss 138.858582\n",
            "Saving checkpoint for epoch 43 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-972\n",
            "Epoch 42 Step 97300 Loss 132.703827\n",
            "Saving checkpoint for epoch 43 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-973\n",
            "Epoch 42 Step 97400 Loss 126.395546\n",
            "Saving checkpoint for epoch 43 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-974\n",
            "Epoch 43 Step 97500 Loss 121.897324\n",
            "Saving checkpoint for epoch 44 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-975\n",
            "Epoch 43 Step 97600 Loss 117.219635\n",
            "Saving checkpoint for epoch 44 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-976\n",
            "Epoch 43 Step 97700 Loss 111.817101\n",
            "Saving checkpoint for epoch 44 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-977\n",
            "Epoch 43 Step 97800 Loss 107.757889\n",
            "Saving checkpoint for epoch 44 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-978\n",
            "Epoch 43 Step 97900 Loss 102.664787\n",
            "Saving checkpoint for epoch 44 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-979\n",
            "Epoch 44 Step 98000 Loss 99.320808\n",
            "Saving checkpoint for epoch 45 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-980\n",
            "Epoch 44 Step 98100 Loss 95.538368\n",
            "Saving checkpoint for epoch 45 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-981\n",
            "Epoch 44 Step 98200 Loss 92.256622\n",
            "Saving checkpoint for epoch 45 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-982\n",
            "Epoch 44 Step 98300 Loss 88.069122\n",
            "Saving checkpoint for epoch 45 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-983\n",
            "Epoch 44 Step 98400 Loss 85.572578\n",
            "Saving checkpoint for epoch 45 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-984\n",
            "Epoch 45 Step 98500 Loss 81.933693\n",
            "Saving checkpoint for epoch 46 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-985\n",
            "Epoch 45 Step 98600 Loss 79.847336\n",
            "Saving checkpoint for epoch 46 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-986\n",
            "Epoch 45 Step 98700 Loss 78.893456\n",
            "Saving checkpoint for epoch 46 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-987\n",
            "Epoch 45 Step 98800 Loss 75.195984\n",
            "Saving checkpoint for epoch 46 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-988\n",
            "Epoch 46 Step 98900 Loss 78.156685\n",
            "Saving checkpoint for epoch 47 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-989\n",
            "Epoch 46 Step 99000 Loss 74.706329\n",
            "Saving checkpoint for epoch 47 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-990\n",
            "Epoch 46 Step 99100 Loss 71.640640\n",
            "Saving checkpoint for epoch 47 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-991\n",
            "Epoch 46 Step 99200 Loss 76.307426\n",
            "Saving checkpoint for epoch 47 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-992\n",
            "Epoch 46 Step 99300 Loss 67.575005\n",
            "Saving checkpoint for epoch 47 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-993\n",
            "Epoch 47 Step 99400 Loss 66.323212\n",
            "Saving checkpoint for epoch 48 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-994\n",
            "Epoch 47 Step 99500 Loss 68.133728\n",
            "Saving checkpoint for epoch 48 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-995\n",
            "Epoch 47 Step 99600 Loss 62.209999\n",
            "Saving checkpoint for epoch 48 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-996\n",
            "Epoch 47 Step 99700 Loss 65.070816\n",
            "Saving checkpoint for epoch 48 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-997\n",
            "Epoch 47 Step 99800 Loss 58.777786\n",
            "Saving checkpoint for epoch 48 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-998\n",
            "Epoch 48 Step 99900 Loss 61.515766\n",
            "Saving checkpoint for epoch 49 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-999\n",
            "Epoch 48 Step 100000 Loss 55.611362\n",
            "Saving checkpoint for epoch 49 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1000\n",
            "Epoch 48 Step 100100 Loss 54.574257\n",
            "Saving checkpoint for epoch 49 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1001\n",
            "Epoch 48 Step 100200 Loss 56.967064\n",
            "Saving checkpoint for epoch 49 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1002\n",
            "Epoch 49 Step 100300 Loss 53.790794\n",
            "Saving checkpoint for epoch 50 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1003\n",
            "Epoch 49 Step 100400 Loss 49.076813\n",
            "Saving checkpoint for epoch 50 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1004\n",
            "Epoch 49 Step 100500 Loss 48.065994\n",
            "Saving checkpoint for epoch 50 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1005\n",
            "Epoch 49 Step 100600 Loss 51.114269\n",
            "Saving checkpoint for epoch 50 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1006\n",
            "Epoch 49 Step 100700 Loss 45.750072\n",
            "Saving checkpoint for epoch 50 at /content/drive/MyDrive/glow/checkpoints/checkpoint_mnist/ckpt-1007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ7Y_5jMwE8I",
        "outputId": "1ed74096-78f2-4284-d2c0-ed1db43dd5ac"
      },
      "source": [
        "from pathlib import Path\n",
        "checkpoint_path=Path('/content/drive/MyDrive/glow/checkpoints/checkpoint_mnist')\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-4) \n",
        "ckpt = tf.train.Checkpoint(model=flow, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
        "                                            checkpoint_path,\n",
        "                                            max_to_keep=10)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('[Flow] Latest checkpoint restored!!')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Flow] Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5haB5j7NO_Y"
      },
      "source": [
        "**Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TnVM-PhR4s6"
      },
      "source": [
        "#test_dataset = test_dataset_raw.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "train_iterator, test_iterator = get_data_alt(1000, 128)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "YwAuunRFRBnt",
        "outputId": "5337c41e-1ae1-417c-f088-c9262b5d8f63"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(10, 2))\n",
        "log_prob_nll = 0\n",
        "for x_test in train_iterator:\n",
        "  inv = flow.bijector.forward(x_test['img'])\n",
        "  re_trg = flow.bijector.inverse(inv)\n",
        "  log_prob_nll += np.abs((tf.reduce_mean(flow.log_prob(inv)) / (np.log(2.) * 32 * 32 * 3)).numpy())\n",
        "\n",
        "\n",
        "print(log_prob_nll)\n",
        "print(log_prob_nll/10000)\n",
        "print('log probability: ', (tf.reduce_mean(flow.log_prob(inv)) / (np.log(2.) * 32 * 32 * 3)).numpy())\n",
        "print(\"inv mean: \", tf.reduce_mean(inv).numpy(), \" std: \", tf.math.reduce_std(inv).numpy())\n",
        "print(\"re:trg mean: \", tf.reduce_mean(re_trg).numpy(), \" std: \", tf.math.reduce_std(re_trg).numpy())\n",
        "\n",
        "def  interporate(a, b, percent):\n",
        "  re_inv = np.array([(percent * a +  (1 - percent) * b)])\n",
        "  re_re_inv = flow.bijector.inverse(re_inv)\n",
        "  out_val = tf.nn.relu6(re_re_inv - tf.reduce_mean(re_re_inv))\n",
        "  #out_norm = (out_val - np.min(out_val)) / (np.max(out_val) - np.min(out_val))\n",
        "  return out_val\n",
        "# re_inv = np.array([(inv[0] + inv[1]) / 2.0])\n",
        "# print(re_inv.shape)\n",
        "#print(tf.reduce_mean(re_inv))\n",
        "# re_re_trg = flow.bijector.inverse(re_inv)\n",
        "\n",
        "re_trg = re_trg.numpy()\n",
        "\n",
        "re_trg = (re_trg + 1) * 127.5\n",
        "re_trg = re_trg.astype(np.uint8)\n",
        "\n",
        "# test_value = interporate(inv[0], inv[1], 0.75)\n",
        "# print(np.min(test_value), np.max(test_value))\n",
        "\n",
        "# test_val = np.random.randn(32,32,3)\n",
        "\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 1)\n",
        "ax.imshow(re_trg[0,:,:,0], aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 2)\n",
        "ax.imshow(interporate(inv[0], inv[1], 0.75)[0,:,:,0], aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 3)\n",
        "ax.imshow(interporate(inv[0], inv[1], 0.5)[0,:,:,0], aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 4)\n",
        "ax.imshow(interporate(inv[0], inv[1], 0.25)[0,:,:,0], aspect=\"auto\", cmap=\"gray_r\")\n",
        "\n",
        "ax = fig.add_subplot(1, 5, 5)\n",
        "ax.imshow(re_trg[1,:,:,0], aspect=\"auto\", cmap=\"gray_r\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79836.34880065918\n",
            "7.983634880065918\n",
            "log probability:  -128.6287\n",
            "inv mean:  -2.2130563  std:  10.696386\n",
            "re:trg mean:  -0.7935118  std:  0.55899763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f63bd7a3ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "XJ0F3YWNt0fm",
        "outputId": "cc2c3e70-00ba-4183-e2b0-96328bf2e6f8"
      },
      "source": [
        "abc = tfp.distributions.Sample(flow)\n",
        "x = abc.sample()\n",
        "x_1 = flow.bijector.forward(tf.reshape(x, (1, 32, 32 ,3)))\n",
        "#print(x)\n",
        "plt.imshow(x, aspect=\"auto\", cmap=\"gray_r\")\n",
        "plt.show()\n",
        "efg = flow.bijector.inverse(x_1)\n",
        "plt.imshow(efg[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbdc6c45898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}